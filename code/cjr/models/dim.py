import z3
import pandas as pd
import numpy as np
import networkx as nx
from pqdict import pqdict
import scipy as sp

try:
    from unionfind import UnionFind
except ImportError:
    from ..utils.unionfind import UnionFind

from datetime import datetime
from collections import defaultdict, OrderedDict
import heapq as PQ
import warnings

if nx.__version__ < '2':
    warnings.warn("Tuned to run only with networkx >= version 2.0rc1,"
                  "current version = {}".format(nx.__version__))


def to_bin(n, length):
    """Convert a number into a vector of booleans corresponding to voting
    pattern of the given length."""
    s = bin(n)[2:]
    s = '0' * (length - len(s)) + s
    return [x == '1' for x in s]


def to_pattern(vote_str):
    """Convert votes expressed as a string to vector of booleans."""
    return [True if v == '+' else False if v == '-' else None
            for v in vote_str]


def to_str(pattern):
    """Converts a True/None/False array into a string."""
    if isinstance(pattern[0], bool):
        return ''.join(['+' if p is True else '-' if p is False else 'o'
                        for p in pattern])
    else:
        return ''.join(['+' if p > 0 else '-' if p < 0 else 'o'
                        for p in pattern])


def mk_vars(num_dims, num_comments, num_voters, ctx=None):
    """Creates and returns variables for the comments and the voters."""
    com_vars = [[z3.Real('x_%d_%d' % (ii, jj), ctx=ctx)
                 for jj in range(num_dims)]
                for ii in range(num_comments)]

    voter_vars = [[z3.Real('v_%d_%d' % (ii, jj), ctx=ctx)
                   for jj in range(num_dims)]
                  for ii in range(num_voters)]

    return com_vars, voter_vars


def add_constr(voter_var, com_var, is_upvote):
    """Creates constraints for a given vote."""
    s = sum(x * v for x, v in zip(voter_var, com_var))
    return s > 0 if is_upvote else s < 0


def sign_mat_to_voting_pats(M):
    """Convert the given matrix to voting patterns."""
    n_rows, n_cols = M.shape
    pat_arr = []
    for j in range(n_cols):
        pat_row = []
        for i in range(n_rows):
            if M[i, j] == 0:
                pat_row.append(None)
            elif M[i, j] > 0:
                pat_row.append(True)
            else:
                pat_row.append(False)
        pat_arr.append(pat_row)

    return pat_arr


def create_prob(n_dim, voting_patterns, ctx=None):
    """Given the (unique) votes, create a problem for Z3 to solve."""
    n_voters = len(voting_patterns)
    n_comments = len(voting_patterns[0])

    if isinstance(voting_patterns[0], str):
        voting_patterns = [to_pattern(x) for x in voting_patterns]

    com_vars, voter_vars = mk_vars(n_dim, n_comments, n_voters, ctx=ctx)
    s = z3.Solver(ctx=ctx)

    cstr_added = set()
    for v_idx, pat in enumerate(voting_patterns):
        for c_idx, vote in enumerate(pat):
            if vote is not None and (v_idx, c_idx, vote) not in cstr_added:
                s.add(add_constr(voter_vars[v_idx], com_vars[c_idx], vote))
                cstr_added.add((v_idx, c_idx, vote))

    for c_idx in range(n_comments):
        s.add(sum([x * x for x in com_vars[c_idx]]) == 1)

    for v_idx in range(n_voters):
        s.add(sum([v * v for v in voter_vars[v_idx]]) == 1)

    return s, com_vars, voter_vars


def create_z3_prob(n_dim, voting_patterns, ctx=None):
    """Alias for create_prob."""
    return create_prob(n_dim=n_dim, voting_patterns=voting_patterns, ctx=ctx)


def make_vectors_from_z3_soln(prob, c_vars, v_vars):
    """Returns c_vecs and v_vecs from a Z3 solution."""
    model = prob.model()

    def get_val(var):
        s = model[var].as_decimal(prec=64)
        if s[-1] == '?':
            # Sometimes the value generated by z3 contains a '?' character
            # as the terminal character.
            # The purpose of the '?' is unclear.
            s = s[:-1]
        return float(s)

    n_commenters = len(c_vars)
    n_dims = len(c_vars[0])
    n_voters = len(v_vars)

    c_vecs = np.zeros((n_commenters, n_dims))
    v_vecs = np.zeros((n_voters, n_dims))

    for i in range(n_commenters):
        for j in range(n_dims):
            c_vecs[i, j] = get_val(c_vars[i][j])

    for i in range(n_voters):
        for j in range(n_dims):
            v_vecs[i, j] = get_val(v_vars[i][j])

    return c_vecs, v_vecs


def gen_voting_patterns(n_comments, n_votes, p_up, p_down, seed=360):
    """Generate random voting patterns with the given probability of
    up/down/no-votes. Note that the generated voting patterns are not forced to
    be unique.
    """
    rs = np.random.RandomState(seed=seed)
    return [to_str(rs.choice([True, False, None], size=n_comments,
                             p=[p_up, p_down, 1 - p_up - p_down]))
            for ii in range(n_votes)]


def opinions_differ(vote_1, vote_2):
    """Returns if the votes differ."""
    return vote_1 != vote_2 if vote_1 != 'o' and vote_2 != 'o' else False


def create_disgreement_graph(num_comments, rand_votes):
    """Create a graph of disagreements among comments as evinced by voting
    patterns."""
    vote_pats = set(rand_votes)
    edges = [(i, j)
             for pat in vote_pats
             for i in range(num_comments)
             for j in range(i + 1, num_comments)
             if opinions_differ(pat[i], pat[j])]
    return nx.Graph(list(set(edges)))


def generate_voting_matrix(num_comments, num_voters, dim, seed=100,
                           voting_probs=None, avg_comments_per_voter=5):
    """Creates voting patterns for users/voters.
    dim: The dimensionality of the opinion space.
    voting_probs: should sum up to 1 and gives the probability of picking each
    comment for voting. Default is Uniform.

    Returns a sparse matrix in LIL format.
    """
    RS = np.random.RandomState(seed)

    # Normalized latent representations.
    # TODO: These are not evenly distributed on a circle, the proper way to do
    # it would be to sample theta_1, theta_2, independently and then use
    # spherical coordinates?
    comment_vecs = RS.rand(num_comments, dim) - 0.5
    comment_vecs = comment_vecs / np.sqrt(np.square(comment_vecs).sum(axis=1))[:, np.newaxis]

    voter_vecs = RS.rand(num_voters, dim) - 0.5
    voter_vecs = voter_vecs / np.sqrt(np.square(voter_vecs).sum(axis=1))[:, np.newaxis]

    if voting_probs is None:
        # Assume that each voter votes on average `avg_comments_per_voter` posts.
        voting_probs = [avg_comments_per_voter / num_comments] * num_comments

    voting_probs = np.asarray(voting_probs)

    M = sp.sparse.lil_matrix((num_comments, num_voters), dtype=np.int8)
    for v_idx in range(num_voters):
        voted_on = np.nonzero(RS.rand(num_comments) < voting_probs)[0]
        voter_vec = voter_vecs[v_idx]

        for c_idx in voted_on:
            vote = comment_vecs[c_idx].dot(voter_vec)
            M[c_idx, v_idx] = np.sign(vote)

    return M, comment_vecs, voter_vecs


def similarity_matrix(comment_vecs, voter_vecs):
    """Creates a matrix with the similarity value between each pair of voter
    and commenter."""
    return comment_vecs.dot(voter_vecs.T)


def get_mat_R2(pred, truth):
    """Calculates the R^2 for the prediction matrix."""
    SS_res = np.square(pred - truth).sum()
    SS_tot = np.square(truth - truth.mean()).sum()
    return 1 - (SS_res / SS_tot)


# ======
# This is the O(n / log n) implementation check.

def same_sign(eq_signs, eq_sets, i, j, col):
    """Returns true if the rows i and j have the same sign on column col.
    Returns false if any of the signs are indeterminate."""
    return eq_signs.get(eq_sets[i, col], 0) * eq_signs.get(eq_sets[j, col], 0) == 1


def diff_signs(eq_signs, eq_sets, i, j, col):
    """Returns true if the rows i and j have different signs on column col.
    Returns false if any of the signs are indeterminate."""
    return eq_signs.get(eq_sets[i, col], 0) * eq_signs.get(eq_sets[j, col], 0) == -1


def mergeable(eq_signs, eq_sets, i, j, col):
    """Determines whether the equivalence classes corresponding to (i, col) and
    (j, col) can be merged."""
    return ((eq_sets[i, col] not in eq_signs) or
            (eq_sets[j, col] not in eq_signs) or
            (eq_signs[eq_sets[i, col]] == eq_signs[eq_sets[j, col]]))


def weight(i, j, P, eq_signs, eq_sets_uf):
    """Calculate the weight of edge (i, j) in sparse matrix M using probabilities vector P."""
    return sum(P[idx]
               for idx in range(len(P))
               if diff_signs(eq_signs, eq_sets_uf, i, j, idx))


def tiebreaker(i, j, len_P, eq_signs, eq_sets_uf):
    """Calculates how many entries row i and j have in common to help in breaking ties between them."""
    return -sum(1 for idx in range(len_P)
                if same_sign(eq_signs, eq_sets_uf, i, j, idx))


def _worker_spanning_tree(params):
    """Worker which does the spanning tree work."""
    ii, jj, probs, eq_sets_uf, eq_signs = params
    weight, tie = 0.0, 0.0

    # Inlining the calculations of weight and tiebreaker for optimization.
    for col in range(len(probs)):
        sign_prod = eq_signs.get(eq_sets_uf[ii, col], 0) * eq_signs.get(eq_sets_uf[jj, col], 0)

        if sign_prod == -1:
            weight += probs[col]
        elif sign_prod == 1:
            tie -= 1.0

    return (weight, tie, (ii, jj))


def make_spanning_tree_old(sign_mat, min_avg=False, pool=None, verbose=False):
    """Create a spanning tree."""
    uf = UnionFind()

    start_time = datetime.now()

    N = sign_mat.shape[0]
    for v in range(N):
        uf.union(v)

    equiv_sets = UnionFind()
    equiv_signs = {(i, j): sign_mat[i, j] for i, j in zip(*sign_mat.nonzero())}

    forest = defaultdict(lambda: set())
    probs = np.ones(sign_mat.shape[1])

    params = [(ii, jj, probs, equiv_sets, equiv_signs)
              for ii in range(N) for jj in range(ii + 1, N)]

    if pool is None:
        edges_heap = [_worker_spanning_tree(x) for x in params]
    else:
        edges_heap = pool.map(_worker_spanning_tree, params)

    PQ.heapify(edges_heap)

    num_edges = 0
    while num_edges < N - 1:
        x, y, (i, j) = PQ.heappop(edges_heap)
        assert i < j

        # Otherwise, this edge may have created a cycle
        if uf[i] != uf[j]:
            edges_forest_i, edges_forest_j = forest[uf[i]], forest[uf[j]]

            new_root = uf.union(i, j)
            forest[new_root] = edges_forest_i.union(edges_forest_j)
            forest[new_root].add((i, j))
            num_edges += 1

            merged_columns = set()

            for col in range(sign_mat.shape[1]):
                if mergeable(equiv_signs, equiv_sets, i, j, col):
                    merged_columns.add(col)
                    # If either of these is undecided, merge an equivalent sets.
                    old_i_root = equiv_sets[i, col]
                    old_j_root = equiv_sets[j, col]

                    root = equiv_sets.union((i, col), (j, col))

                    if old_i_root in equiv_signs:
                        equiv_signs[root] = equiv_signs[old_i_root]
                    elif old_j_root in equiv_signs:
                        equiv_signs[root] = equiv_signs[old_j_root]

                    # Which edges can be ignored due to column 'col'?

            if not min_avg:
                # Minimize the worst case.
                # probs = [(2 * p / (1 + x))
                #              if diff_signs(equiv_signs, equiv_sets, i, j, idx)
                #              else p / (1 + x)
                #          for idx, p in enumerate(probs)]
                probs = [(2 * p) if diff_signs(equiv_signs, equiv_sets, i, j, idx) else p
                         for idx, p in enumerate(probs)]
            else:
                # Minimize the average case.
                probs = [1
                         if diff_signs(equiv_signs, equiv_sets, i, j, idx)
                         else 0
                         for idx, p in enumerate(probs)]

            # This can probably be optimized further because update of the weights in
            # this manner was required for the proof.
            # It should be possible to reduce it since we are only doubling
            # some entries in probs since the division does not change the order of
            # any other entries in the matrices.
            #
            # As it stands, this is an O(M^2 * N) operation.
            params = [
                (ii, jj, probs, equiv_sets, equiv_signs)
                for ii in range(N)
                for jj in range(ii + 1, N)
                if uf[ii] != uf[jj]
                # Optimization to avoid cycles: from O(N^2) => O((N - num_edges)^2) per iteration
            ]

            if verbose:
                cur_time = datetime.now()
                print('edge = {}, added: ({}, {}), (x, y) = ({}, {}, {}), elapsed = {}sec'
                      .format(num_edges, i, j, x, y, (cur_time - start_time).total_seconds()))

            if pool is None:
                edges_heap = [_worker_spanning_tree(w) for w in params]
            else:
                edges_heap = pool.map(_worker_spanning_tree, params)

            PQ.heapify(edges_heap)

            # if verbose:
            #     differing_cols = [idx for idx in range(len(probs))
            #                       if diff_signs(equiv_signs, equiv_sets, i, j, idx)]
            #     print(sorted(edges_heap)[:5])
            #     # print('differing_columns = ', differing_cols, 'merged columns = ', merged_columns)
            #     # print([edges_heap[idx] for idx in range(len(edges_heap)) if edges_heap[idx][2][0] == 0 and edges_heap[idx][2][1] == 3])
            #     print('***************\n')

    return forest[uf[0]], equiv_sets, equiv_signs


def make_spanning_tree(sign_mat, min_avg=False, pool=None, verbose=False, disable_tiebreaker=False):
    """Create a spanning tree."""
    uf = UnionFind()

    start_time = datetime.now()

    N = sign_mat.shape[0]
    for v in range(N):
        uf.union(v)

    equiv_sets = UnionFind()
    eq_set_pos = defaultdict(lambda: set())
    equiv_signs = {(i, j): sign_mat[i, j] for i, j in zip(*sign_mat.nonzero())}

    col_sets = [{1: set(), -1: set()} for _ in range(sign_mat.shape[1])]

    for i, j in zip(*sign_mat.nonzero()):
        col_sets[j][equiv_signs[i, j]].add(i)

    forest_edges = defaultdict(lambda: set())

    probs = np.ones(sign_mat.shape[1])
    params = [(ii, jj, probs, equiv_sets, equiv_signs)
              for ii in range(N) for jj in range(ii + 1, N)]

    if pool is None:
        edges_heap = [_worker_spanning_tree(x) for x in params]
    else:
        edges_heap = pool.map(_worker_spanning_tree, params)

    pq_dict = pqdict({(i, j): (wt, wt, ties, (i, j)) for (wt, ties, (i, j)) in edges_heap})

    def update_col_sets(row, col, sgn, old_pos):
        all_old_pos = old_pos.union([row])

        loop_1_size = len(all_old_pos) * len(col_sets[col][-1 * sgn])
        loop_2_size = len(pq_dict)

        if loop_1_size < loop_2_size:
            for u_ in col_sets[col][-1 * sgn]:
                for v_ in all_old_pos:
                    # These edges now have an additional differing
                    # column.
                    u, v = min(u_, v_), max(u_, v_)
                    if uf[u] != uf[v] and (u, v) in pq_dict:
                        (wt, orig_wt, tie, (_, _)) = pq_dict[u, v]
                        pq_dict[u, v] = (wt + probs[col], orig_wt, tie, (u, v))
        else:
            set_1 = col_sets[col][-1 * sgn]
            set_2 = all_old_pos

            for u, v in pq_dict:
                if uf[u] != uf[v]:
                    if (u in set_1 and v in set_2) or (v in set_1 and u in set_2):
                        (wt, orig_wt, tie, (_, _)) = pq_dict[u, v]
                        pq_dict[u, v] = (wt + probs[col], orig_wt, tie, (u, v))
                else:
                    del pq_dict[u, v]

        if not disable_tiebreaker:
            loop_1_size = len(all_old_pos) * len(col_sets[col][sgn])
            loop_2_size = len(pq_dict)

            if loop_1_size < loop_2_size:
                for u_ in col_sets[col][sgn]:
                    for v_ in all_old_pos:
                        # These edges now have an additional matching
                        # column.
                        u, v = min(u_, v_), max(u_, v_)
                        if uf[u] != uf[v] and (u, v) in pq_dict:
                            (wt, orig_wt, tie, (_, _)) = pq_dict[u, v]
                            pq_dict[u, v] = (wt, orig_wt, tie - 1, (u, v))
            else:
                set_1 = col_sets[col][-1 * sgn]
                set_2 = all_old_pos

                for u, v in pq_dict:
                    if uf[u] != uf[v]:
                        if (u in set_1 and v in set_2) or (v in set_1 and u in set_2):
                            (wt, orig_wt, tie, (_, _)) = pq_dict[u, v]
                            pq_dict[u, v] = (wt, orig_wt, tie - 1, (u, v))
                    else:
                        del pq_dict[u, v]

        col_sets[col][sgn].update(all_old_pos)

    num_edges = 0
    while num_edges < N - 1:
        (_, _), (wt, orig_wt, ties, (i, j)) = pq_dict.popitem()
        assert i < j

        # Otherwise, this edge may have created a cycle
        if uf[i] != uf[j]:
            differing_columns = set(col for col in range(sign_mat.shape[1])
                                    if diff_signs(equiv_signs, equiv_sets, i, j, col))
            hot_columns = differing_columns
            merged_columns = set()

            edges_forest_i, edges_forest_j = forest_edges[uf[i]], forest_edges[uf[j]]
            new_root = uf.union(i, j)

            forest_edges[new_root] = edges_forest_i.union(edges_forest_j)
            forest_edges[new_root].add((i, j))

            # # Remove all the edges which can no longer be part of the spanning
            # # tree.
            # removed_edges = 0
            # for u_ in nodes_forest_i:
            #     for v_ in nodes_forest_j:
            #         u, v = min(u_, v_), max(u_, v_)
            #         if (u, v) in pq_dict:
            #             del pq_dict[u, v]
            #             removed_edges += 1

            # if removed_edges and verbose:
            #     print("{} edges removed.".format(removed_edges))

            num_edges += 1

            for col in range(sign_mat.shape[1]):
                if mergeable(equiv_signs, equiv_sets, i, j, col):
                    merged_columns.add(col)

                    # If either of these is undecided, merge an equivalent sets.
                    old_i_root, old_j_root = equiv_sets[i, col], equiv_sets[j, col]
                    old_i_pos, old_j_pos = eq_set_pos[old_i_root], eq_set_pos[old_j_root]

                    old_i_sign_set = old_i_root in equiv_signs
                    old_j_sign_set = old_j_root in equiv_signs

                    root = equiv_sets.union((i, col), (j, col))
                    eq_set_pos[root] = old_i_pos.union(old_j_pos).union([i, j])

                    if old_i_sign_set:
                        equiv_signs[root] = equiv_signs[old_i_root]
                        if not old_j_sign_set:
                            update_col_sets(j, col, equiv_signs[root], old_j_pos)
                    elif old_j_sign_set:
                        equiv_signs[root] = equiv_signs[old_j_root]
                        if not old_i_sign_set:
                            update_col_sets(i, col, equiv_signs[root], old_i_pos)
                    # if verbose:
                    #     for row in range(N):
                    #         eq_set = equiv_sets[row, col]
                    #         if eq_set in equiv_signs:
                    #             assert row in col_sets[col][equiv_signs[eq_set]]

            if not min_avg:
                # Minimize the worst case.
                for col in differing_columns:
                    for u_ in col_sets[col][1]:
                        for v_ in col_sets[col][-1]:
                            u, v = min(u_, v_), max(u_, v_)
                            # Increase the contribution of this column in the weight
                            if uf[u] != uf[v] and (u, v) in pq_dict:
                                (wt, orig_wt, tie, (_, _)) = pq_dict[u, v]
                                pq_dict[u, v] = (wt + probs[col], orig_wt, tie, (u, v))

                    # Double the weight of this column.
                    probs[col] *= 2
            else:
                # raise NotImplementedError()
                # # Minimize the average case.
                # probs = [1 if col in differing_columns else 0
                #          for col, p in enumerate(probs)]
                # Not changing the probs at all.
                pass

            # This can probably be optimized further because update of the weights in
            # this manner was required for the proof.
            # It should be possible to reduce it since we are only doubling
            # some entries in probs since the division does not change the order of
            # any other entries in the matrices.
            #
            # As it stands, this is an O(M^2 * N) operation.

            # params = [
            #     (ii, jj, probs, equiv_sets, equiv_signs)
            #     for ii in range(N)
            #     for jj in range(ii + 1, N)
            #     if uf[ii] != uf[jj]
            #     # Optimization to avoid cycles: from O(N^2) => O((N - num_edges)^2) per iteration
            # ]

            if verbose:
                cur_time = datetime.now()
                print('edge = {}, hot_columns = {}, added: ({}, {}), (wt, orig_wt, ties) = ({}, {}, {}), possible: {}, elapsed = {}sec'
                      .format(num_edges, len(hot_columns), i, j, wt, orig_wt, ties, len(pq_dict), (cur_time - start_time).total_seconds()))

            # if pool is None:
            #     edges_heap = [_worker_spanning_tree(y) for y in params]
            # else:
            #     edges_heap = pool.map(_worker_spanning_tree, params)

            # pq_dict.update({(u, v): (weight, tie, (u, v)) for (weight, tie, (u, v)) in edges_heap})

            # if verbose:
            #     for u in range(N):
            #         for v in range(sign_mat.shape[1]):
            #             if uf[u] == uf[v] and (u, v) in pq_dict:
            #                 del pq_dict[u, v]

            #     n_smallest = nsmallest(5, pq_dict)
            #     print(sorted((pq_dict[k]) for k in n_smallest))
            #     # print('differing_columns = ', differing_columns, 'merged columns = ', merged_columns)
            #     print('***************\n')

    return forest_edges[uf[0]], equiv_sets, equiv_signs


def make_spanning_tree_full(full_sign_mat, min_avg=False, pool=None, verbose=False):
    """Create a spanning tree for a full sign matrix."""
    uf = UnionFind()

    start_time = datetime.now()

    N = full_sign_mat.shape[0]
    n_cols = full_sign_mat.shape[1]

    for v in range(N):
        uf.union(v)

    forest = defaultdict(lambda: set())
    probs = np.ones(n_cols)
    equiv_signs = full_sign_mat

    params = [(ii, jj) for ii in range(N) for jj in range(ii + 1, N)]

    global _worker_spanning_tree_full

    def _worker_spanning_tree_full(params):
        """Worker which does the spanning tree work."""
        ii, jj, = params
        weight, tie = 0.0, 0.0

        # Inlining the calculations of weight and tiebreaker for optimization.
        for col in range(len(probs)):
            sign_prod = equiv_signs[ii, col] * equiv_signs[jj, col]

            if sign_prod == -1:
                weight += probs[col]
            elif sign_prod == 1:
                tie -= 1.0

        return (weight, tie, (ii, jj))

    if pool is None:
        edges_heap = [_worker_spanning_tree_full(x) for x in params]
    else:
        edges_heap = pool.map(_worker_spanning_tree_full, params)

    col_sets = [{1: set(np.where(full_sign_mat[:, col] == 1)[0]),
                 -1: set(np.where(full_sign_mat[:, col] == -1)[0])}
                for col in range(n_cols)]

    # for i, j in zip(*sign_mat.nonzero()):
    #     col_sets[j][equiv_signs[i, j]].add(i)

    pq_dict = pqdict({(i, j): (x, y, (i, j)) for (x, y, (i, j)) in edges_heap})

    num_edges = 0
    while num_edges < N - 1:
        (_, _), (x, y, (i, j)) = pq_dict.popitem()

        if uf[i] != uf[j]:
            differing_columns = set(col for col in range(n_cols)
                                    if full_sign_mat[i, col] != full_sign_mat[j, col])

            edges_forest_i, edges_forest_j = forest[uf[i]], forest[uf[j]]
            new_root = uf.union(i, j)
            forest[new_root] = edges_forest_i.union(edges_forest_j)
            forest[new_root].add((i, j))
            num_edges += 1

            if not min_avg:
                for col in differing_columns:
                    for u_ in col_sets[col][1]:
                        for v_ in col_sets[col][-1]:
                            u, v = min(u_, v_), max(u_, v_)
                            if uf[u] != uf[v] and (u, v) in pq_dict:
                                (wt, tie, (_, _)) = pq_dict[u, v]
                                # Double the weight
                                pq_dict[u, v] = (wt + probs[col], tie, (u, v))

                    # Double the weight of this column.
                    probs[col] *= 2
            else:
                pass

            if verbose:
                cur_time = datetime.now()
                print('edge = {}, hot_columns = {}, added: ({}, {}), (x, y) = ({}, {}), possible: {}, elapsed = {}sec'
                      .format(num_edges, len(differing_columns), i, j, x, y, len(pq_dict), (cur_time - start_time).total_seconds()))

    return forest[uf[0]]


def make_graph(spanning_tree):
    """Returns a networkx graph for the spanning tree."""
    graph = nx.Graph()
    graph.add_edges_from(spanning_tree)
    return graph


def get_M_full(M_sparse, equiv_sets, equiv_signs):
    """Creates a full matrix based on deductions done."""
    M_full = np.zeros(M_sparse.shape)
    for i in range(M_sparse.shape[0]):
        for j in range(M_sparse.shape[1]):
            M_full[i, j] = equiv_signs.get(equiv_sets[i, j], 0)
    return M_full


def make_one_permut(spanning_tree, source=None):
    """Creates a path from the spanning tree.

    For M nodes, there can be any number of possible Eulerian paths, ranging
    from 1 to (M - 1)!.

    TODO: Christofide's algorithm can be used here for the spanning tree to
    Eulerian path reduction.
    """
    G = nx.MultiGraph()

    # Adding two copies of the edges to 'G' to make it Eulerian.
    G.add_edges_from(spanning_tree)
    G.add_edges_from(spanning_tree)

    permut = [None] * (len(spanning_tree) + 1)
    permut_idx = 0
    handled_nodes = set()

    for (u, v) in nx.eulerian_circuit(G, source=source):
        if u not in handled_nodes:
            handled_nodes.add(u)
            permut[permut_idx] = u
            permut_idx += 1

        if v not in handled_nodes:
            handled_nodes.add(v)
            permut[permut_idx] = v
            permut_idx += 1

    return permut


def SC(M_full, permut):
    """Calculates the maximum number of sign changes in M_full."""
    M_permut = M_full[permut, :]
    sc = 0
    max_cols = []

    for j in range(M_permut.shape[1]):
        col_sc = 0
        for i in range(1, M_permut.shape[0]):
            if M_permut[i, j] != M_permut[i - 1, j]:
                col_sc += 1

        if col_sc == sc:
            max_cols.append(j)
        elif col_sc > sc:
            max_cols = [j]
            sc = col_sc

    return sc, max_cols


# Polynomial algorithm to figure out if minrank(S) <= 2

def make_S_prime_full(S_full, as_set=False):
    """Creates S_prime given an S sign matrix.
    This is used as a routine inside check_minrank_full.
    """
    S = np.sign(S_full).copy()  # TODO: One extra copy?

    # Making the first column of S all +1
    for row in range(S.shape[0]):
        if S[row, 0] == -1:
            S[row, :] *= -1

    # Get rid of duplicate rows
    S = np.unique(S, axis=0)

    # Get rid of duplicate columns
    S = np.unique(S, axis=1)

    # Add missing complements to S
    n_rows, n_cols = S.shape

    col_to_sign = {tuple(S[:, col]) for col in range(n_cols)}

    missing_complements = []
    for col in range(n_cols):
        col_sign_complement = tuple(-1 * S[:, col])
        if col_sign_complement not in col_to_sign:
            missing_complements.append(col_sign_complement)

    # Creating S_prime
    if not as_set:
        S_prime = np.zeros((n_rows, len(missing_complements) + n_cols), dtype='int8')
        S_prime[:, :n_cols] = S
        S_prime[:, n_cols:] = np.asarray(missing_complements).T
        return S_prime
    else:
        return col_to_sign.union(missing_complements)


def check_minrank_low_full(S_full):
    """Calculates whether the sign-rank(S) <= 2 or not in polynomial time.
    The matrix S needs to be full (i.e. no missing data).
    """

    assert np.sum(S_full == 0) == 0, "S_full must be filled in."

    S_prime = make_S_prime_full(S_full)
    n_rows, n_cols = S_prime.shape

    S_prime_set = {tuple(S_prime[:, i]) for i in range(n_cols)}

    # print(S_prime_set)
    # A_seq = sorted(S_prime_set, reverse=True, key=lambda x: sum(1 for y in x if y == -1))

    k = len(S_prime_set) / 2

    assert int(k) == k

    i = 2
    Ai = set()
    Ais = [Ai]
    while i <= k + 1:
        A_min_len = 0
        A_mins = []
        min_col = None

        for col in S_prime_set:
            minus_locs = {idx for idx, s in enumerate(col) if s == -1}

            if Ai.issubset(minus_locs) and len(minus_locs) > len(Ai):
                if A_min_len > len(minus_locs) or len(A_mins) == 0:
                    A_mins = [minus_locs]
                    A_min_len = len(minus_locs)
                    min_col = col
                elif A_min_len == len(minus_locs):
                    A_mins.append(minus_locs)

        if len(A_mins) > 1 or len(A_mins) == 0:
            # More than one candidate or no candidate was found.
            # => The min-rank is greater than 2
            if i == 2:
                # Breaking the tie for A_2
                pass
            else:
                # print(A_mins, k, i)
                return False, Ais, S_prime

        A_min = A_mins[0]

        if len(A_min) == S_full.shape[0] and i < k:
            # All indexes have already been covered.
            return False, Ais, S_prime

        S_prime_set.remove(min_col)
        Ai = A_min
        Ais.append(Ai)
        i += 1

    return True, Ais, S_prime


def compl_tup(t):
    """Produce the complement of a tuple."""
    return tuple(-x for x in t)


def make_S_prime(S):
    """Files in the missing complement rows of S.
    Returns a dense matrix.
    """
    # Will work with sparse matrices afterwards, after verifying that this
    # algorithm actually works

    # O(n * m)
    if sp.sparse.isspmatrix(S):
        S = S.toarray()

    S_prime = make_S_prime_full(S)

    # Map complement indexes.

    compl_map = {}
    col_map = {tuple(S_prime[:, col_idx]): col_idx
               for col_idx in range(S_prime.shape[1])}
    for col in col_map:
        compl_map[col_map[col]] = col_map[compl_tup(col)]

    return S_prime, compl_map


def make_graph_cols(S):
    """Creates a graph of S's columns based on how they will appear
    in the T(S) set.

    Creating a DAG of all the columns in the tree such that all nodes which
    have a '-' point to nodes which contain a '+' in that place. This will
    allow us to select nodes in increasing order of '-'s for Ais.
    """
    n_rows, n_cols = S.shape

    col_graph = nx.DiGraph()
    col_graph.add_nodes_from(range(n_cols))

    for i in range(n_cols):
        for j in range(i + 1, n_cols):
            i_ge_j = True
            j_ge_i = True
            for k in range(n_rows):
                if S[k, i] == 1 and S[k, j] == -1:
                    i_ge_j = False
                elif S[k, j] == 1 and S[k, i] == -1:
                    j_ge_i = False

            if i_ge_j:
                col_graph.add_edge(j, i)
            elif j_ge_i:
                col_graph.add_edge(i, j)

    return col_graph


def get_minus_locs(M, col):
    """Returns the row indexes where the column has -ve sign."""
    return {idx for idx in range(M.shape[0]) if M[idx, col] == -1}


def get_plus_locs(M, col):
    """Returns the row indexes where the column has -ve sign."""
    return {idx for idx in range(M.shape[0]) if M[idx, col] == +1}


def check_3_col_low_rank(S, verbose=False):
    """Takes three columns and determines if they have a low dimension.
    Only works for full rank matrices.
    """
    # Step 1: Find all sign-patterns in the first two column which have
    #         different signs in the third column.
    # Step 2: Check if the number of unique sign-patterns is either 1, or 2 but
    #         the sign patterns are complements of each other.
    n_rows, n_cols = S.shape

    changed = False
    contradiction = False

    S = S.copy()
    for i in range(n_cols):
        for j in range(i + 1, n_cols):
            for k in range(n_cols):
                if k == i or k == j:
                    # i, j, k must be distinct columns.
                    continue

                # Full patterns where the kth column is +
                pos_set = set((S[row, i], S[row, j])
                              for row in range(n_rows)
                              if S[row, k] > 0 and S[row, i] * S[row, j] != 0)

                # Full patterns where the kth column is -
                neg_set = set((S[row, i], S[row, j])
                              for row in range(n_rows)
                              if S[row, k] < 0 and S[row, i] * S[row, j] != 0)

                split_pats = (pos_set.intersection(neg_set)
                              .union((x, y) for x, y in pos_set if (-x, -y) in pos_set)
                              .union((x, y) for x, y in neg_set if (-x, -y) in neg_set))

                if len(split_pats) > 0:
                    # Add complements of the patterns to the set.
                    split_pats = split_pats.union({(-x, -y) for x, y in split_pats})

                if len(split_pats) == 0:
                    # Could not find any patterns in (i, j) which were "split" by column k
                    continue
                elif len(split_pats) > 2:
                    # Found more than one split pattern
                    contradiction = True
                    break # Break out of k
                else:
                    # Now we can pattern match and fill in S for all patterns
                    # except the split_pats we have found.
                    if verbose:
                        print('Split patterns = {}, ({}, {}, {})'
                              .format(list(map(to_str, split_pats)), i, j, k))

                    pat_match = {}
                    for x, y, z in zip(S[:, i], S[:, j], S[:, k]):
                        if x * y * z != 0:
                            if (x, y) not in split_pats:
                                pat_match[x, y] = z
                                pat_match[-x, -y] = -z

                    for row in range(n_rows):
                        x, y = S[row, i], S[row, j]
                        if (x, y) in pat_match and S[row, k] == 0:

                            if verbose:
                                print('Changing [{}, {}] to {}'
                                      .format(row, k, pat_match[x, y]))

                            S[row, k] = pat_match[x, y]
                            changed = True

            if contradiction:
                break # Break out of j

        if contradiction:
            break # Break out of i

    if contradiction:
        if verbose:
            print('(i, j, k) = ({}, {}, {}); split_pats = {}'
                  .format(i, j, k, list(map(to_str, split_pats))))
        return False, S
    elif changed:
        return check_3_col_low_rank(S, verbose=verbose)
    else:
        return True, S


def fill_randomly_low_rank(S, verbose=False, guesses=None):
    """Use the low rank assumption to fill in the given sparse matrix."""
    res, S_ = check_3_col_low_rank(S, verbose=False)

    if not res:
        if verbose:
            print("S is not minrank.")
        return False, guesses, S_

    res, S__ = check_3_col_low_rank(S_.T, verbose=False)
    S__ = S__.T

    if not res:
        if verbose:
            print("S_ is not minrank.")
        return False, guesses, S__

    if guesses is None:
        guesses = OrderedDict()

    if S__.nonzero()[0].shape == np.prod(S.shape):
        return True, guesses, S__

    # Pick first location
    x, y = np.argwhere(S__ == 0)[0]

    S_guess = S__.copy()
    S_guess[x, y] = 1
    guesses[x, y] = 1
    res, guesses_, S_filled = fill_randomly_low_rank(S_guess, verbose=verbose,
                                                     guesses=guesses)

    if not res:
        # Since 1 did not work, use -1 instead.
        if verbose:
            print('Branch mispredict!')

        S_guess[x, y] = -1
        guesses[x, y] = -1
        res, guesses_, S_filled = fill_randomly_low_rank(S_guess, verbose=verbose,
                                                         guesses=guesses)

    return res, guesses_, S_filled


def check_minrank_low(S, seed=99, verbose=False):
    """Checks whether there is a completion of matrix S such that minrank(S) <= 2."""
    # First need to ensure that all the columns have their complements.

    # rs = np.random.RandomState(seed)
    S_prime, compl_map = make_S_prime(S)
    S_prime = S_prime.astype('int8')

    n_rows, n_cols = S_prime.shape

    col_graph = make_graph_cols(S_prime)

    exploration_order = [x[0]
                         for x in sorted(col_graph.in_degree(),
                                         key=lambda x: x[1])]

    if verbose:
        print("Exploration order: ", exploration_order)

    S_prime_set = {tuple(S_prime[:, col_idx]): col_idx
                   for col_idx in range(n_cols)}

    k = len(S_prime_set) / 2

    found_path = False
    source_node_idx = 0

    for root in exploration_order:
        minus_locs = get_minus_locs(S_prime, root)
        fringe = [((), set(), minus_locs, root)]

        while len(fringe) > 0:
            path, path_set, Ai, next_col = fringe.pop()  # Depth first

            if len(path) == k - 1:
                # Found a path of length 'k', complements can be filled in now.
                found_path = path + (next_col,)
                break

            Ai = Ai.union(get_minus_locs(S_prime, next_col))

            for _, dst in col_graph.out_edges(next_col):
                if dst not in path_set and compl_map[dst] not in path_set:
                    if len(get_plus_locs(S_prime, dst).intersection(Ai)) == 0:
                        fringe.append((path + (next_col,), path_set.union([next_col]), Ai, dst))

                # and compl_map[dst] not in path_set and dst != compl_map[next_col])

        if found_path:
            break

        source_node_idx += 1

    if found_path:
        Ai = set()
        Ais = [Ai]
        if verbose:
            print('Path = ', found_path)

        for col in found_path:
            Ai = Ai.union({idx for idx in range(n_rows) if S_prime[idx, col] == -1})

            for idx in Ai:
                assert S_prime[idx, col] != 1
                S_prime[idx, col] = -1

                assert S_prime[idx, compl_map[col]] != -1
                S_prime[idx, compl_map[col]] = 1

            S_prime[:, col] = np.where(S_prime[:, col] == 0, 1, S_prime[:, col])
            S_prime[:, compl_map[col]] = np.where(S_prime[:, compl_map[col]] == 0, -1, S_prime[:, compl_map[col]])
            Ais.append(Ai)
        return True, Ais, S_prime, source_node_idx
    else:
        return False, None, None, source_node_idx

    # Ai = set()
    # Ais = []
    # while True:
    #     A_min_len = 0
    #     A_mins = []
    #     min_cols = []
    #     col_idxs = []

    #     for searched_idx in seq_Ais_idx:
    #         col_idx = seq_Ais_idx[searched_idx]
    #         if col_idx in covered_col_idx or col_idx in covered_compl_idx:
    #             continue

    #         col = tuple(S_prime[:, col_idx])

    #         unfixed_locs = {idx for idx, s in enumerate(col) if s == 0}
    #         minus_locs = {idx for idx, s in enumerate(col) if s == -1}

    #         if Ai.issubset(minus_locs.union(unfixed_locs)):
    #             # Now find out how many elements in the column would be set to
    #             # '-' in all.
    #             locs_to_minus = Ai.union(minus_locs)

    #             if A_min_len > len(locs_to_minus) or len(A_mins) == 0:
    #                 A_mins = [locs_to_minus]
    #                 A_min_len = len(locs_to_minus)
    #                 min_cols = [col]
    #                 col_idxs = [seq_Ais_idx[searched_idx]]
    #             elif A_min_len == len(locs_to_minus):
    #                 A_mins.append(locs_to_minus)
    #                 min_cols.append(col)
    #                 col_idxs.append(seq_Ais_idx[searched_idx])

    #     if len(A_mins) == 0:
    #         break

    #     # Now there may be multiple choices here, we select a random one.
    #     # chosen_idx = rs.randint(0, len(A_mins))

    #     # Now we choose the first matched element according to the topological
    #     # sort.
    #     chosen_idx = 0
    #     A_min = A_mins[chosen_idx]
    #     min_col = min_cols[chosen_idx]
    #     # col_idx = S_prime_set[min_col]
    #     col_idx = col_idxs[chosen_idx]

    #     covered_col_idx.add(col_idx)
    #     covered_compl_idx.add(compl_map[col_idx])

    #     # unfixed_locs = {idx for idx, s in enumerate(min_col) if s == 0}
    #     # minus_locs = {idx for idx, s in enumerate(min_col) if s == -1}
    #     locs_set_minus = A_min

    #     if verbose:
    #         print('i = {} Removing column = {} A_min_len = {}, col_idx = {}, col_map = {}'
    #               .format(i, min_col, A_min_len, col_idx, compl_map[col_idx]))
    #     # Calculate the new complement, after all the missing entries are
    #     # filled with 1.
    #     filled_col = tuple(-1 if idx in locs_set_minus else +1
    #                        for idx in range(n_rows))
    #     S_prime[:, col_idx] = filled_col

    #     new_compl_col = compl_tup(filled_col)
    #     S_prime[:, compl_map[col_idx]] = new_compl_col

    #     # Remove old column
    #     # S_prime_set[filled_col] = S_prime_set[min_col]
    #     # del S_prime_set[min_col]

    #     # old_compl_col = compl_tup(min_col)
    #     # if old_compl_col in S_prime_set:
    #     #     # TODO: Not sure why we need this check.
    #     #     # We needed this check because now columns need not be unique.
    #     #     S_prime_set[new_compl_col] = S_prime_set[old_compl_col]
    #     #     del S_prime_set[old_compl_col]
    #     # else:
    #     #     if verbose:
    #     #         print('Not found in S_prime_set: ', old_compl_col)

    #     Ai = A_min
    #     Ais.append(Ai)
    #     i += 1

    # # return len(Ais[-1]) == n_rows and len(Ais) == k, Ais, S_prime
    # return len(Ais) >= k, Ais, S_prime

def make_df_from_M_generic(key_name, key_value, M):
    """Returns a data-frame with the given key, value pair."""
    data = []
    for c, v in zip(*M.nonzero()):
        data.append({
            key_name: key_value,
            'comment_id': c,
            'voter_id': v,
            'vote_type': np.sign(M[c, v])
        })

    return pd.DataFrame.from_dict(data)


def make_df_from_M(comment_tree_id, M):
    """Returns a data-frame with the given comment tree-id."""
    return make_df_from_M_generic(key_name='comment_tree_id',
                                  key_value=comment_tree_id,
                                  M=M)


def make_M_from_df_generic(df, key_name, key_value, return_maps=False):
    """Creates a M matrix from the key_{name,value} pair."""
    df = df[df[key_name] == key_value]

    voters = {v: idx for idx, v in enumerate(sorted(df.voter_id.unique()))}
    comments = {v: idx for idx, v in enumerate(sorted(df.comment_id.unique()))}

    M = sp.sparse.lil_matrix((len(comments), len(voters)), dtype='int8')
    for v_id, c_id, vote in df[['voter_id', 'comment_id', 'vote_type']].values:
        M[comments[c_id], voters[v_id]] = vote

    if return_maps:
        return M, voters, comments
    else:
        return M


def make_M_from_df(df, comment_tree_id):
    """Creates a M matrix from the comment_tree.
    Returns a sparse matrix in the LIL format.
    """
    return make_M_from_df_generic(df,
                                  key_name='comment_tree_id',
                                  key_value=comment_tree_id)


def _make_test_lowrank_params(seed):
    """Create the arguments needed for testing lowrank."""
    rs = np.random.RandomState(seed)
    dim = 2 if rs.rand() < 0.5 else rs.randint(2, 10)
    n_rows = rs.randint(dim, 10 * dim)
    n_cols = rs.randint(dim, 10 * dim)

    M, c_vecs, v_vecs = generate_voting_matrix(n_rows, n_cols, dim=dim, seed=2 * seed)
    M_full = np.sign(c_vecs.dot(v_vecs.T))

    return dim, M, M_full


def test_lowrank(seed, verbose=False):
    """Testing lowrank implementations."""

    dim, M, M_full = _make_test_lowrank_params(seed=seed)
    is_sparse_lowrank, _, M_arbit_filled, root_node_idx = check_minrank_low(M, seed=seed * 2, verbose=verbose)
    is_full_lowrank, _, _ = check_minrank_low_full(M_full)

    if is_full_lowrank and dim > 2:
        print("false positive at seed = ", seed)
        return seed, "false positive"
    elif not is_full_lowrank and dim <= 2:
        print("false negative at seed = ", seed)
        return seed, "false negative"

    if is_sparse_lowrank and dim > 2:
        print('false positive in sparse at seed = ', seed, ' dim = ', M_arbit_filled.shape, M.shape)
        is_filled_lowrank, _, _ = check_minrank_low_full(M_arbit_filled)
        if not is_filled_lowrank:
            print('filled matrix not low-rank at seed = ', seed)
            return seed, "error in filling"
        return seed, "sparse false positive"

    elif not is_sparse_lowrank and dim <= 2:
        print("false negative in sparse at seed = ", seed, ' dims = ', M_arbit_filled.shape, M.shape)
        return seed, "sparse false negative"

    if is_sparse_lowrank and dim <= 2 and root_node_idx > 0:
        print("root_node_idx = {} for seed = {}".format(root_node_idx, seed))

    return seed, "OK"


def test_lowrank_3_fill(seed, verbose=False):
    """Testing lowrank implementations."""

    dim, M, M_full = _make_test_lowrank_params(seed=seed)

    if verbose:
        print('Dim = {}, Shape = {}'.format(dim, M.shape))

    is_sparse_lowrank, guesses, M_arbit_filled = fill_randomly_low_rank(M.toarray(), verbose=verbose)
    is_full_lowrank, _, _ = check_minrank_low_full(M_full)

    if is_full_lowrank and dim > 2:
        print("false positive at seed = {}, dim = {}, shape = {}"
              .format(seed, dim, M.shape))
        return seed, "false positive"
    elif not is_full_lowrank and dim <= 2:
        print("false negative at seed = {}, dim = {}, shape = {}"
              .format(seed, dim, M.shape))
        return seed, "false negative"

    if is_sparse_lowrank and dim > 2:
        print('false positive in sparse at seed = ', seed, ' dim = ', M_arbit_filled.shape)
        is_filled_lowrank, _, _ = check_minrank_low_full(M_arbit_filled)
        if not is_filled_lowrank:
            print('filled matrix not low-rank at seed = ', seed)
            return seed, "error in filling"
        return seed, "sparse false positive"
    elif not is_sparse_lowrank and dim <= 2:
        print("false negative in sparse at seed = ", seed, ' dims = ', M_arbit_filled.shape, M.shape)
        return seed, "sparse false negative"

    if not is_sparse_lowrank and dim >= 2 and len(guesses) > 0:
        print("guesses needed = {} for seed = {}, dim = {}, shape = {]"
              .format(guesses, seed, dim, M.shape))

    return seed, "OK"
